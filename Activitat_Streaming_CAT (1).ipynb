{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d611a6f1",
   "metadata": {},
   "source": [
    "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", as well as your name and collaborators below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21422e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"Guillem Mata Valligny\"\n",
    "COLLABORATORS = \"Enric Sintes Arguimbau\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4caef4",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b86be79",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "84a35fab47838c3c46566bee56da35a1",
     "grade": false,
     "grade_id": "cell-c37c14bf24e0ec8d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Activitat 2: Streaming - Adquisició i anàlisi de dades"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c499d2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "33a1a54aa2b58ff2bf837463bb0cfab2",
     "grade": false,
     "grade_id": "cell-758412ed8ae90120",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Introducció\n",
    "\n",
    "En aquesta activitat, explorarem dues de les tecnologies més populars utilitzades per a l'adquisició i l'anàlisi de dades en temps real: Apache Kafka i Apache Spark Streaming. A través d'una sèrie d'exercicis, aprendréu els conceptes bàsics d'aquestes tecnologies i com utilitzar-les per processar dades en temps real.\n",
    "\n",
    "## Estructura\n",
    "\n",
    "Hem dividit aquesta activitat en tres parts:\n",
    "\n",
    "1.  **Introducció a Apache Kafka**: En aquesta primera part, aprendréu els conceptes bàsics d'Apache Kafka, \n",
    "    els seus principals components i com interactuar-hi mitjançant la interfície de línia de comandes.\n",
    "2.  **Ingesta de dades amb Apache Kafka**: En aquesta segona part, aprendréu a utilitzar Apache Kafka per \n",
    "    ingerir dades des d'un productor i consumir-les des d'un consumidor.\n",
    "3.  **Processament de dades en temps real amb Apache Spark Streaming:** En aquesta tercera part, aprendréu \n",
    "    a utilitzar Apache Spark Streaming per processar dades en temps real provinents d'Apache Kafka.\n",
    "\n",
    "## Notes importants:\n",
    "\n",
    "- L'activitat s'ha de realitzar en **grups de 2 membres**. \n",
    "  Assegureu-vos de saber qui és el vostre company abans de començar l'activitat.\n",
    "- Tot i que és possible completar les activitats directament en aquest quadern, **desaconsellem fer-ho** \n",
    "  degut a possibles problemes de rendiment del servidor. Veureu que cada activitat està continguda en la \n",
    "  seva pròpia cel·la, la qual cosa permetrà copiar-la fàcilment a un arxiu Python. Aquest arxiu es pot \n",
    "  **executar en un servidor utilitzant SSH o VSCode**. Un cop hagueu executat i provat l'script amb èxit, \n",
    "  simplement copieu-lo de nou a la cel·la corresponent del quadern. Aquest enfocament garanteix una execució \n",
    "  més fluida i una millor gestió dels recursos del servidor.\n",
    "- En alguns exercicis, necessitareu **prendre captures de pantalla per justificar les vostres respostes**. Podeu     capturar imatges utilitzant les eines del sistema operatiu que feu servir: \"Recortes\" a Windows, \"Imprimir         pantalla\", `Ctrl+C` en seleccionar una imatge, etc. Un cop capturades, podeu enganxar les imatges directament a     les cel·les de resposta utilitzant `Ctrl+V` o el menú contextual que apareix fent clic dret, que permetrà           enganxar la imatge del portapapers. Per visualitzar la imatge, heu d'executar la cel·la.\n",
    "- **Heu d'utilitzar únicament les biblioteques proporcionades, excepte si s'indica el contrari.**\n",
    "- Si us plau, no canvieu el nom del quadern ni el tipus de les cel·les.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb08530",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0e251e2d69f817d323a39061331dc9e2",
     "grade": false,
     "grade_id": "cell-d293ed241598a70c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Part I: Introducció a Apache Kafka (2 punts)\n",
    "\n",
    "[Kafka](https://kafka.apache.org) és una plataforma distribuïda per gestionar esdeveniments en streaming que ens permet llegir, escriure i processar esdeveniments (registre o missatges, segons la terminologia de Kafka) distribuïts a través d'un clúster. \n",
    "\n",
    "Començarem l'activitat creant un tema anomenat activity2<usuari> en el servei de Kafka del nostre clúster (substitueix <usuari> pel teu nom d'usuari). Un tema és una col·lecció ordenada d'esdeveniments que s'emmagatzema de forma persistent, generalment en disc, i es distribueix i replica. Kafka tracta cada tema en cada partició com un registre (un conjunt ordenat de missatges). Cada missatge en una partició té un desplaçament únic, i aquests missatges tenen un **període de retenció predeterminat de 7 dies (604,800,000 ms)**, tot i que pots modificar-lo en el moment de la creació del tema.\n",
    "\n",
    "Kafka funciona basat en [Zookeeper](https://zookeeper.apache.org), que gestiona clústers per proporcionar serveis de coordinació a aplicacions distribuïdes. Zookeeper està ubicat en el servidor al qual et connectes, és a dir, en localhost, i el seu port és el 2181. Els brokers de Kafka són **Cloudera02 i Cloudera03**, accessibles pel port estàndard **9092**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1650335e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "002d4f5d2c65032e848b89b471ac84da",
     "grade": false,
     "grade_id": "cell-7cad57cad27779ae",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Exercici 1: Crear un tema amb Kafka (0.25 punts)\n",
    "\n",
    "Crea un tema de Kafka anomenat `activity2<usuari>` en el nostre clúster amb un factor de replicació de 1 i una única partició, el que significa que farem servir un sol node per emmagatzemar els missatges que rep Kafka. A més, especifica que els missatges només s'emmagatzemin durant 2 hores en el tema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79647b0c",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f2d50ddf9aa1acd51a2f52a42c93f50d",
     "grade": true,
     "grade_id": "cell-ec6b56255dffcc3c",
     "locked": false,
     "points": 0.25,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from kafka.admin import KafkaAdminClient, NewTopic, ConfigResource\n",
    "\n",
    "topic_name = \"activity2gmatav\"\n",
    "\n",
    "admin_client = KafkaAdminClient(\n",
    "    bootstrap_servers=\"Cloudera02:9092\", \n",
    "    client_id='test'\n",
    ")\n",
    "\n",
    "# Creació del tema, factor de replicació 1 , única partició\n",
    "topic_config = {'retention.ms': '7200000'}  # Missatges només s'emmagatzemen durant 2 hores en el tema\n",
    "topic = NewTopic(name=topic_name, num_partitions=1, replication_factor=1, topic_configs=topic_config)\n",
    "admin_client.create_topics(new_topics=[topic])\n",
    "\n",
    "admin_client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23e84b8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ff5f17776e215c4b5578b70329945285",
     "grade": false,
     "grade_id": "cell-6c2e29722b9cd99b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Exercici 2: Llista els temes de Kafka (0.25 punts)\n",
    "\n",
    "Consulta el tema que acabes de crear i mostra'l."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c62a0a4",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fe96623764634c99675a6ea9f695c9d2",
     "grade": true,
     "grade_id": "cell-b05732c7c474ceb8",
     "locked": false,
     "points": 0.25,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/kafka/client_async.py\u001b[0m in \u001b[0;36mwakeup\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    931\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 932\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wake_w\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msendall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mb'x'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    933\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 9] Bad file descriptor",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-a67fa3a5e502>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Consutem el tema i mostra per pantalla\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtopic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madmin_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescribe_topics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"activity2gmatav\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0madmin_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/kafka/admin/client.py\u001b[0m in \u001b[0;36mdescribe_topics\u001b[0;34m(self, topics)\u001b[0m\n\u001b[1;32m    516\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdescribe_topics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 518\u001b[0;31m         \u001b[0mmetadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_cluster_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtopics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    519\u001b[0m         \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'topics'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/kafka/admin/client.py\u001b[0m in \u001b[0;36m_get_cluster_metadata\u001b[0;34m(self, topics, auto_topic_creation)\u001b[0m\n\u001b[1;32m    505\u001b[0m         future = self._send_request_to_node(\n\u001b[1;32m    506\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleast_loaded_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 507\u001b[0;31m             \u001b[0mrequest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    508\u001b[0m         )\n\u001b[1;32m    509\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait_for_futures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfuture\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/kafka/admin/client.py\u001b[0m in \u001b[0;36m_send_request_to_node\u001b[0;34m(self, node_id, request)\u001b[0m\n\u001b[1;32m    362\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mexception\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmessage\u001b[0m \u001b[0mcould\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mbe\u001b[0m \u001b[0msent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m         \"\"\"\n\u001b[0;32m--> 364\u001b[0;31m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    365\u001b[0m             \u001b[0;31m# poll until the connection to broker is ready, otherwise send()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m             \u001b[0;31m# will fail with NodeNotReadyError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/kafka/client_async.py\u001b[0m in \u001b[0;36mready\u001b[0;34m(self, node_id, metadata_priority)\u001b[0m\n\u001b[1;32m    404\u001b[0m             \u001b[0mbool\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mwe\u001b[0m \u001b[0mare\u001b[0m \u001b[0mready\u001b[0m \u001b[0mto\u001b[0m \u001b[0msend\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mgiven\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m         \"\"\"\n\u001b[0;32m--> 406\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaybe_connect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    407\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_ready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata_priority\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetadata_priority\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/kafka/client_async.py\u001b[0m in \u001b[0;36mmaybe_connect\u001b[0;34m(self, node_id, wakeup)\u001b[0m\n\u001b[1;32m    340\u001b[0m             \u001b[0;31m# the client lock in poll().\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mwakeup\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwakeup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    343\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/kafka/client_async.py\u001b[0m in \u001b[0;36mwakeup\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    935\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mKafkaTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    936\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 937\u001b[0;31m                 \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unable to send to wakeup socket!'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    938\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    939\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_clear_wake_fd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/logging/__init__.py\u001b[0m in \u001b[0;36mwarning\u001b[0;34m(self, msg, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1289\u001b[0m         \"\"\"\n\u001b[1;32m   1290\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misEnabledFor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWARNING\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1291\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_log\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWARNING\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1293\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/logging/__init__.py\u001b[0m in \u001b[0;36m_log\u001b[0;34m(self, level, msg, args, exc_info, extra, stack_info)\u001b[0m\n\u001b[1;32m   1413\u001b[0m         record = self.makeRecord(self.name, level, fn, lno, msg, args,\n\u001b[1;32m   1414\u001b[0m                                  exc_info, func, extra, sinfo)\n\u001b[0;32m-> 1415\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/logging/__init__.py\u001b[0m in \u001b[0;36mhandle\u001b[0;34m(self, record)\u001b[0m\n\u001b[1;32m   1423\u001b[0m         \"\"\"\n\u001b[1;32m   1424\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisabled\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1425\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallHandlers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1427\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0maddHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhdlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/logging/__init__.py\u001b[0m in \u001b[0;36mcallHandlers\u001b[0;34m(self, record)\u001b[0m\n\u001b[1;32m   1484\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mhdlr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandlers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1485\u001b[0m                 \u001b[0mfound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfound\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1486\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlevelno\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mhdlr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1487\u001b[0m                     \u001b[0mhdlr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1488\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpropagate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Consutem el tema i mostra per pantalla\n",
    "topic = admin_client.describe_topics([\"activity2gmatav\"])\n",
    "print(topic)\n",
    "admin_client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1c6bd7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "caea97d96603a15c871e312958583489",
     "grade": false,
     "grade_id": "cell-d5d24f1092688463",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Exercici 3: Elimina el tema de Kafka (0.25 punts)\n",
    "\n",
    "Elimina el *tema* que vas crear en l'Exercici 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c357a3c",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "00823c545d6787273c0b04c7efa2d21f",
     "grade": true,
     "grade_id": "cell-0fabd287d5e83b34",
     "locked": false,
     "points": 0.25,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeleteTopicsResponse_v3(throttle_time_ms=0, topic_error_codes=[(topic='activity2gmatav', error_code=0)])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Eliminar el tema creat\n",
    "admin_client.delete_topics(topics=['activity2gmatav'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ae0639",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "efcf0c52904848aac4ed95d03a2e413f",
     "grade": false,
     "grade_id": "cell-316cdff2cc7e17df",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Exercici 4: Descriu el tema de Kafka (0.25 punts)\n",
    "\n",
    "Torna a crear el *tema* anomenat activity2<usuari> seguint els mateixos passos que en l'Exercici 1. Un cop creat, utilitza la línia de comandes de Kafka per descriure'l i comprovar-ne la configuració."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ebc816b6",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d5226e2a1a309282405195947f8cb205",
     "grade": true,
     "grade_id": "cell-62b105cd150b41b5",
     "locked": false,
     "points": 0.25,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic:activity2gmatav\tPartitionCount:1\tReplicationFactor:1\tConfigs:retention.ms=7200000\n",
      "\tTopic: activity2gmatav\tPartition: 0\tLeader: 95\tReplicas: 95\tIsr: 95\n"
     ]
    }
   ],
   "source": [
    "from kafka.admin import KafkaAdminClient, NewTopic, ConfigResource\n",
    "\n",
    "# Tornem a crear el tema \n",
    "topic_name = \"activity2gmatav\"\n",
    "\n",
    "\n",
    "admin_client = KafkaAdminClient(\n",
    "    bootstrap_servers=\"Cloudera02:9092\", \n",
    "    client_id='test'\n",
    ")\n",
    "\n",
    "topic_config = {'retention.ms': '7200000'}  \n",
    "topic = NewTopic(name=topic_name, num_partitions=1, replication_factor=1, topic_configs=topic_config)\n",
    "admin_client.create_topics(new_topics=[topic])\n",
    "\n",
    "# Descriu el tema creat amb linea de comandes Kafka\n",
    "!/opt/cloudera/parcels/CDH-6.2.0-1.cdh6.2.0.p0.967373/lib/kafka/bin/kafka-topics.sh \\\n",
    "--describe \\\n",
    "--topic activity2gmatav \\\n",
    "--zookeeper Cloudera01:2181/kafka"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fca2272",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "739f25e3ba6e544fd921971cbb30e6b1",
     "grade": false,
     "grade_id": "cell-c83b99fd0989f458",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Exercici 5: Crea un productor en Kafka (0.5 punts)\n",
    "\n",
    "Anem a crear un esdeveniment en el tema. Recorda que aquest comandament s'ha d'executar des de la terminal per interactuar. Recorda utilitzar `CTRL+c` quan hagis acabat d'enviar els missatges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fe183ae1",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cc830f39d29f559d5b2cca4b06588527",
     "grade": true,
     "grade_id": "cell-86aa21dca9ad2af3",
     "locked": false,
     "points": 0.5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">^C\n",
      "org.apache.kafka.common.KafkaException: Producer closed while send in progress\n",
      "\tat org.apache.kafka.clients.producer.KafkaProducer.doSend(KafkaProducer.java:864)\n",
      "\tat org.apache.kafka.clients.producer.KafkaProducer.send(KafkaProducer.java:841)\n",
      "\tat kafka.tools.ConsoleProducer$.send(ConsoleProducer.scala:75)\n",
      "\tat kafka.tools.ConsoleProducer$.main(ConsoleProducer.scala:57)\n",
      "\tat kafka.tools.ConsoleProducer.main(ConsoleProducer.scala)\n",
      "Caused by: org.apache.kafka.common.KafkaException: Requested metadata update after close\n",
      "\tat org.apache.kafka.clients.Metadata.awaitUpdate(Metadata.java:200)\n",
      "\tat org.apache.kafka.clients.producer.KafkaProducer.waitOnMetadata(KafkaProducer.java:981)\n",
      "\tat org.apache.kafka.clients.producer.KafkaProducer.doSend(KafkaProducer.java:861)\n",
      "\t... 4 more\n"
     ]
    }
   ],
   "source": [
    "/opt/cloudera/parcels/CDH-6.2.0-1.cdh6.2.0.p0.967373/lib/kafka/bin/kafka-console-producer.sh \\--broker-list Cloudera02:9092 \\--topic activity2gmatav"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1574e8d2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a1f60945f581772a1400ceeed3775985",
     "grade": false,
     "grade_id": "cell-053c2241e4c3f039",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Exercici 6: Crea un consumidor a Kafka (0.5 punts)\n",
    "\n",
    "Finalment, es demana consultar els missatges enviats a través de la terminal utilitzant el consumidor incorporat de Kafka per al tema. Concretament, has d'executar un consumidor connectant-te als diferents brokers existents i especificant el tema i la partició als quals s'han enviat els missatges. Pots obrir dues terminals i verificar que els missatges enviats amb el productor al broker poden ser visualitzats amb el consumidor de consola."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f3e4ceb4",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b64a5d9bcb059f236624f5e8073b13c9",
     "grade": true,
     "grade_id": "cell-097f3fa710421948",
     "locked": false,
     "points": 0.5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hola, Kafka!\n",
      "Hola KAFKA !!!!\n",
      "QUe tal EStas\n",
      "Jo estic Be\n",
      "^C\n",
      "Processed a total of 4 messages\n"
     ]
    }
   ],
   "source": [
    "/opt/cloudera/parcels/CDH-6.2.0-1.cdh6.2.0.p0.967373/lib/kafka/bin/kafka-console-consumer.sh \\\n",
    "--bootstrap-server Cloudera02:9092 \\\n",
    "--topic activity2gmatav \\\n",
    "--from-beginning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a3d1ba",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "578a856a2d6436837932c9491b23f5ea",
     "grade": false,
     "grade_id": "cell-4e3eb51a468a7e25",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Part II: Ingesta de dades con Apache Kafka (1.5 punts)\n",
    "\n",
    "Per automatitzar la generació i consum de dades, és comú treballar amb un llenguatge de programació com Python, en lloc de fer-ho directament en Bash. En les següents preguntes, explorarem la funcionalitat de Kafka utilitzant Python amb la biblioteca per defecte, que **NO HAGIS D'INSTAL·LAR, ja que ja està disponible en la versió correcta.**\n",
    "Pots trobar tota la documentació associada amb l'API proporcionada a [Kafka](https://kafka-python.readthedocs.io/en/master/). \n",
    "Començarem amb els conceptes bàsics que ja hem cobert: escriure en el tema de Kafka. \n",
    "Per fer-ho, configurarem [Kafka\n",
    "producer](https://kafka-python.readthedocs.io/en/master/apidoc/KafkaProducer.html) producer que inserirà valors numèrics en un tema de Kafka cada 3 segons. Mentre el productor escriu, procedirem a llegir els missatges en la pregunta 8."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd264762",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "600ea8d22a012a69c1aab4483b6d6927",
     "grade": false,
     "grade_id": "cell-1ece0f1a8eb7b62c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Exercici 7: Escriu un tema a Kafka (0.5 punts)\n",
    "\n",
    "Se't demana escriure una seqüència numèrica de 300 nombres (del 1 al 300) en el tema de Kafka activity2<usuari> que acabem de crear. Cada un dels missatges escrits en el tema ha de contenir informació sobre el tema on s'escriuen, una clau i el valor binari del nombre a escriure (per exemple, value=b'287'). \n",
    "\n",
    "És essencial revisar l'API associada amb el [Kafka producer](https://kafka-python.readthedocs.io/en/master/apidoc/KafkaProducer.html)per completar aquests exercicis.\n",
    "\n",
    "Per guiar-te en l'exercici, et proporcionem una plantilla que pots utilitzar per completar-ho. Has de completar les parts mancants del codi a la cel·la de codi a continuació.\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f43acf7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d58dd164d1266d805040690609048479",
     "grade": false,
     "grade_id": "cell-12dab48155c333c1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from kafka import KafkaProducer\n",
    "import numpy as np\n",
    "<FILL_IN>\n",
    "for i in range(1,300):\n",
    "    <FILL_IN>\n",
    "producer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8eec72ac",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1f66ccec2eedcafcdea59166358c92cd",
     "grade": true,
     "grade_id": "cell-17580c624ff2aa62",
     "locked": false,
     "points": 0.5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from kafka import KafkaProducer\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "producer = KafkaProducer(\n",
    "    bootstrap_servers=['Cloudera02:9092'],\n",
    "    key_serializer=lambda k: k.encode('utf-8'), \n",
    "    value_serializer=lambda v: v.encode('utf-8') \n",
    ")\n",
    "\n",
    "\n",
    "for i in range(1, 301):\n",
    "    key = 'key-{}'.format(i)  \n",
    "    value = '{}'.format(i)  \n",
    "    producer.send('activity2gmatav', key=key, value=value)\n",
    "\n",
    "producer.flush()\n",
    "producer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb4be6b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7d044b060d580ac9c1d6426398674db9",
     "grade": false,
     "grade_id": "cell-73d5b980eb6f8e12",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Exercici 8: Llegir un tema de Kafka (1 punt)\n",
    "\n",
    "Utilitzant la biblioteca de Python per a Kafka, \n",
    "[Kafka](https://kafka-python.readthedocs.io/en/master/),\n",
    "llegeix els missatges enviats en l'exercici 7, mostrant només els valors, \n",
    "no les altres propietats del missatge. És important revisar l'ús de Kafka en \n",
    "([Python](https://kafka-python.readthedocs.io/en/master/usage.html))i els paràmetres del consumidor de \n",
    "[Kafka\n",
    "consumer](https://kafka-python.readthedocs.io/en/master/apidoc/KafkaConsumer.html). consumer.\n",
    "\n",
    "Com en l'exercici anterior, aquí et proporcionem una plantilla que pots utilitzar per completar l'exercici. Has de completar les parts mancants del codi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c09cfdd",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c0f2b632680feba17e8414c875a9edbe",
     "grade": false,
     "grade_id": "cell-56844d262bc72fb4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from kafka import KafkaConsumer\n",
    "<FILL_IN>\n",
    "for message in consumer:\n",
    "    <FILL_IN>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "001eb2ef",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "51e344e77dd90c924bbc77e661c69763",
     "grade": true,
     "grade_id": "cell-093c5a1a7e6ef493",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hola KAFKA !!!!\n",
      "QUe tal EStas\n",
      "Jo estic Be\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n"
     ]
    }
   ],
   "source": [
    "from kafka import KafkaConsumer\n",
    "\n",
    "\n",
    "consumer = KafkaConsumer('activity2gmatav',\n",
    "                         group_id='GrupStreaming5',\n",
    "                         bootstrap_servers=['Cloudera02:9092'],\n",
    "                         consumer_timeout_ms=10000\n",
    "                        )\n",
    "# Mostrem els valors dels missatges enviats\n",
    "for message in consumer:\n",
    "   \n",
    "   print(message.value.decode('utf-8'))\n",
    "                                          "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85362c83",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fbf7fa66b846a8910725f42fe7e2859e",
     "grade": false,
     "grade_id": "cell-1189e62b37db4192",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Part III: Procesament de dades en temps real amb Apache Spark Streaming (6.5 punts)\n",
    "\n",
    "\n",
    "En aquesta part de l'activitat, ens centrarem en l'ús d'Apache Spark per processar dades en temps real. Aquí, utilitzarem:\n",
    "\n",
    "- [Spark Streaming](https://archive.apache.org/dist/spark/docs/2.4.0/streaming-programming-guide.html): \n",
    "  Spark Streaming és un motor de processament de fluxos escalable i tolerant a fallades, construït sobre Apache       Spark. Permet el processament de dades en temps real amb alta capacitat de rendiment i baixa latència. \n",
    "  Amb Spark Streaming, pots realitzar anàlisis en temps real, aprenentatge automàtic i processament de grafos sobre   dades en streaming.\n",
    "- [Spark Structured Streaming](https://archive.apache.org/dist/spark/docs/2.4.0/structured-streaming-programming-     guide.html): Structured Streaming és una API d'alt nivell per al processament de fluxos en Spark. \n",
    "  Proporciona una interfície declarativa i similar a SQL per processar dades de streaming estructurades. \n",
    "  Amb Structured Streaming, pots escriure consultes de streaming que s'integren de manera fluida amb el               processament per lots, permetent-te construir pipelines de processament de dades de punta a punta.\n",
    "\n",
    "En aquesta part de l'activitat, ens centrarem en la xarxa social Mastodon per processar les seves dades en streaming. És una bona idea familiaritzar-se amb l'estructura JSON d'un \"toot\".(consulta el link [Mastodon\\'s API\n",
    "webpage](https://docs.joinmastodon.org/entities/Status/)), per facilitar la comprensió dels diferents exercicis que proposem.\n",
    "\n",
    "**Notes importants:**\n",
    "\n",
    "- Tingues en compte que, en utilitzar la versió 2.4.0 d'Apache Spark, pot haver-hi diferències en l'API i altres característiques en comparació amb les versions més recents. Assegura't de consultar la documentació específica per a aquesta versió quan necessitis més detalls.\n",
    "\n",
    "- Recorda adjuntar una captura de pantalla del resultat de l'execució de cada exercici en aquesta part de l'activitat.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f34925",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "364e1f9a51cb66a55a9cb68aa29a4efd",
     "grade": false,
     "grade_id": "cell-d370b874b6fb9f3d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Exercici 10: Spark Streaming (3.5 punts)\n",
    "\n",
    "En aquest exercici, analitzaràs l'activitat a Mastodon comptant els toots en una finestra de temps. Comptar és una de les operacions fonamentals en Spark, i en aquesta activitat, aprofitarem la biblioteca de Spark Streaming per millorar les nostres capacitats de recompte."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55a340a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "95a15568f0a0040d8adec5ce4883a8ea",
     "grade": false,
     "grade_id": "cell-0268954ec18f3d70",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "### Exercici 10.1: Counting in Windows (0.5 punts)\n",
    "\n",
    "Com ja sabràs, la biblioteca Spark Streaming **processa les dades utilitzant el concepte de time windows**, agrupant els elements de dades segons el temps en què van ser rebuts. Aquest enfocament permet el processament per lots de dades de streaming, habilitant anàlisis **sobre intervals de temps diferents**. Veuràs que la sintaxi per realitzar operacions sobre els RDDs dins d'aquestes finestres de temps és **pràcticament equivalent a les operacions estàndard d'RDD que ja coneixes.**\n",
    "\n",
    "Completa el codi a continuació per obtenir **el nombre de toots originals publicats cada cinc segons**. Exclou els retweets del teu recompte. Potser necessitaràs consultar [Mastodon\n",
    "API](https://docs.joinmastodon.org/entities/Status/) per entendre com estan estructurats els toots.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d42ae935",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2024-12-04 20:59:55\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2024-12-04 21:00:00\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2024-12-04 21:00:05\n",
      "-------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import json \n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "\n",
    "    \n",
    "# Initialize SparkContext and StreamingContext with a 1-second batch interval\n",
    "app_name = \"NombreDeToots\"  # Name of your application\n",
    "\n",
    "# Create the SparkContext\n",
    "try:\n",
    "    sc = SparkContext(\"local[2]\", appName = app_name)\n",
    "except ValueError:\n",
    "    sc.stop()\n",
    "    sc = SparkContext(\"local[2]\", appName = app_name)\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "\n",
    "# Create the StreamingContext\n",
    "batch_interval = 5  # Batch interval in seconds\n",
    "ssc = StreamingContext(sc, batch_interval)\n",
    "ssc.checkpoint(\"checkpoint\")  # Necessary for updateStateByKey operation\n",
    "\n",
    "# Define Kafka parameters\n",
    "kafka_server = \"Cloudera02:9092\"  # Kafka server address\n",
    "kafka_topic = \"mastodon_toots\"   # Kafka topic\n",
    "kafka_group = \"GrupStreaming5\"   # Kafka consumer group, first surname of each member of the group separated by an underscore.\n",
    "\n",
    "kafkaParams = {\n",
    "    \"metadata.broker.list\": kafka_server,\n",
    "    \"group.id\": kafka_group\n",
    "}\n",
    "\n",
    "# Create a DStream that connects to Kafka\n",
    "kafkaStream = KafkaUtils.createDirectStream(  ssc, [kafka_topic],kafkaParams)\n",
    "\n",
    "\n",
    "# Count each toot as 1 and update the total count\n",
    "tootCounts = kafkaStream \\\n",
    "    .map(lambda x: json.loads(x[1])) \\\n",
    "    .filter(lambda toot: not toot.get(\"reblog\")) \\\n",
    "    .map(lambda toot: (\"toot\", 1)) \\\n",
    "    .reduceByKeyAndWindow(lambda a, b: a + b, lambda a, b: a - b, 5, 5)\n",
    "\n",
    "# Print the cumulative count\n",
    "tootCounts.pprint()\n",
    "\n",
    "# Start the computation\n",
    "try:\n",
    "    ssc.start()\n",
    "    ssc.awaitTermination()\n",
    "except KeyboardInterrupt:\n",
    "    ssc.stop()\n",
    "    sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0fe49f",
   "metadata": {},
   "source": [
    "Adjunta la captura de pantalla de la sortida **aquí**"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "83cf1321",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30120c9b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "76d11b3aaa73323ed3de4672fd375703",
     "grade": false,
     "grade_id": "cell-9b4854b7aca5fd3a",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "### Exercici 10.2: Comptant Toots per Idioma (0.5 punts)\n",
    "\n",
    "\n",
    "Com vas observar a l'Exercici 2.1, el procés és força similar a treballar amb RDDs. Ara, anem a aprofundir en una anàlisi més complexa per **comptar quants toots originals es creen per idioma cada 5 segons**. Per millorar la llegibilitat, et demanem que ordenis els idiomes en ordre descendent segons el nombre de toots i limitis la sortida als 10 principals idiomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e093e87d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/opt/cloudera/parcels/CDH-6.2.0-1.cdh6.2.0.p0.967373/lib/spark/python/pyspark/serializers.py\", line 587, in dumps\n",
      "    return cloudpickle.dumps(obj, 2)\n",
      "  File \"/opt/cloudera/parcels/CDH-6.2.0-1.cdh6.2.0.p0.967373/lib/spark/python/pyspark/cloudpickle.py\", line 863, in dumps\n",
      "    cp.dump(obj)\n",
      "  File \"/opt/cloudera/parcels/CDH-6.2.0-1.cdh6.2.0.p0.967373/lib/spark/python/pyspark/cloudpickle.py\", line 260, in dump\n",
      "    return Pickler.dump(self, obj)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 408, in dump\n",
      "    self.save(obj)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 725, in save_tuple\n",
      "    save(element)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/opt/cloudera/parcels/CDH-6.2.0-1.cdh6.2.0.p0.967373/lib/spark/python/pyspark/cloudpickle.py\", line 406, in save_function\n",
      "    self.save_function_tuple(obj)\n",
      "  File \"/opt/cloudera/parcels/CDH-6.2.0-1.cdh6.2.0.p0.967373/lib/spark/python/pyspark/cloudpickle.py\", line 549, in save_function_tuple\n",
      "    save(state)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 810, in save_dict\n",
      "    self._batch_setitems(obj.items())\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 836, in _batch_setitems\n",
      "    save(v)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 770, in save_list\n",
      "    self._batch_appends(obj)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 797, in _batch_appends\n",
      "    save(tmp[0])\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 520, in save\n",
      "    self.save_reduce(obj=obj, *rv)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 623, in save_reduce\n",
      "    save(state)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 810, in save_dict\n",
      "    self._batch_setitems(obj.items())\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 836, in _batch_setitems\n",
      "    save(v)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 520, in save\n",
      "    self.save_reduce(obj=obj, *rv)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 623, in save_reduce\n",
      "    save(state)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n",
      "    f(self, obj) # Call unbound method with explicit self\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 810, in save_dict\n",
      "    self._batch_setitems(obj.items())\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 836, in _batch_setitems\n",
      "    save(v)\n",
      "  File \"/usr/lib/python3.5/pickle.py\", line 495, in save\n",
      "    rv = reduce(self.proto)\n",
      "  File \"/opt/cloudera/parcels/CDH-6.2.0-1.cdh6.2.0.p0.967373/lib/spark/python/pyspark/context.py\", line 321, in __getnewargs__\n",
      "    \"It appears that you are attempting to reference SparkContext from a broadcast \"\n",
      "Exception: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o1731.start.\n: java.io.IOException: org.apache.spark.SparkException: An exception was raised by Python:\nTraceback (most recent call last):\n  File \"/opt/cloudera/parcels/CDH-6.2.0-1.cdh6.2.0.p0.967373/lib/spark/python/pyspark/serializers.py\", line 587, in dumps\n    return cloudpickle.dumps(obj, 2)\n  File \"/opt/cloudera/parcels/CDH-6.2.0-1.cdh6.2.0.p0.967373/lib/spark/python/pyspark/cloudpickle.py\", line 863, in dumps\n    cp.dump(obj)\n  File \"/opt/cloudera/parcels/CDH-6.2.0-1.cdh6.2.0.p0.967373/lib/spark/python/pyspark/cloudpickle.py\", line 260, in dump\n    return Pickler.dump(self, obj)\n  File \"/usr/lib/python3.5/pickle.py\", line 408, in dump\n    self.save(obj)\n  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/usr/lib/python3.5/pickle.py\", line 725, in save_tuple\n    save(element)\n  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/opt/cloudera/parcels/CDH-6.2.0-1.cdh6.2.0.p0.967373/lib/spark/python/pyspark/cloudpickle.py\", line 406, in save_function\n    self.save_function_tuple(obj)\n  File \"/opt/cloudera/parcels/CDH-6.2.0-1.cdh6.2.0.p0.967373/lib/spark/python/pyspark/cloudpickle.py\", line 549, in save_function_tuple\n    save(state)\n  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/usr/lib/python3.5/pickle.py\", line 810, in save_dict\n    self._batch_setitems(obj.items())\n  File \"/usr/lib/python3.5/pickle.py\", line 836, in _batch_setitems\n    save(v)\n  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/usr/lib/python3.5/pickle.py\", line 770, in save_list\n    self._batch_appends(obj)\n  File \"/usr/lib/python3.5/pickle.py\", line 797, in _batch_appends\n    save(tmp[0])\n  File \"/usr/lib/python3.5/pickle.py\", line 520, in save\n    self.save_reduce(obj=obj, *rv)\n  File \"/usr/lib/python3.5/pickle.py\", line 623, in save_reduce\n    save(state)\n  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/usr/lib/python3.5/pickle.py\", line 810, in save_dict\n    self._batch_setitems(obj.items())\n  File \"/usr/lib/python3.5/pickle.py\", line 836, in _batch_setitems\n    save(v)\n  File \"/usr/lib/python3.5/pickle.py\", line 520, in save\n    self.save_reduce(obj=obj, *rv)\n  File \"/usr/lib/python3.5/pickle.py\", line 623, in save_reduce\n    save(state)\n  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/usr/lib/python3.5/pickle.py\", line 810, in save_dict\n    self._batch_setitems(obj.items())\n  File \"/usr/lib/python3.5/pickle.py\", line 836, in _batch_setitems\n    save(v)\n  File \"/usr/lib/python3.5/pickle.py\", line 495, in save\n    rv = reduce(self.proto)\n  File \"/opt/cloudera/parcels/CDH-6.2.0-1.cdh6.2.0.p0.967373/lib/spark/python/pyspark/context.py\", line 321, in __getnewargs__\n    \"It appears that you are attempting to reference SparkContext from a broadcast \"\nException: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/opt/cloudera/parcels/CDH-6.2.0-1.cdh6.2.0.p0.967373/lib/spark/python/pyspark/streaming/util.py\", line 115, in dumps\n    func.func, func.rdd_wrap_func, func.deserializers)))\n  File \"/opt/cloudera/parcels/CDH-6.2.0-1.cdh6.2.0.p0.967373/lib/spark/python/pyspark/serializers.py\", line 597, in dumps\n    raise pickle.PicklingError(msg)\n_pickle.PicklingError: Could not serialize object: Exception: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.\n\n\tat org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1336)\n\tat org.apache.spark.streaming.api.python.TransformFunction.writeObject(PythonDStream.scala:100)\n\tat sun.reflect.GeneratedMethodAccessor19.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:1140)\n\tat java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1496)\n\tat java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)\n\tat java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)\n\tat java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)\n\tat java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)\n\tat java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)\n\tat java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)\n\tat java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)\n\tat java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)\n\tat java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)\n\tat java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)\n\tat java.io.ObjectOutputStream.writeArray(ObjectOutputStream.java:1378)\n\tat java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1174)\n\tat java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)\n\tat java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)\n\tat java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)\n\tat java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)\n\tat java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)\n\tat java.io.ObjectOutputStream.defaultWriteObject(ObjectOutputStream.java:441)\n\tat org.apache.spark.streaming.DStreamGraph$$anonfun$writeObject$1.apply$mcV$sp(DStreamGraph.scala:187)\n\tat org.apache.spark.streaming.DStreamGraph$$anonfun$writeObject$1.apply(DStreamGraph.scala:182)\n\tat org.apache.spark.streaming.DStreamGraph$$anonfun$writeObject$1.apply(DStreamGraph.scala:182)\n\tat org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1329)\n\tat org.apache.spark.streaming.DStreamGraph.writeObject(DStreamGraph.scala:182)\n\tat sun.reflect.GeneratedMethodAccessor66.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:1140)\n\tat java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1496)\n\tat java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)\n\tat java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)\n\tat java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)\n\tat java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)\n\tat java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)\n\tat java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)\n\tat java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348)\n\tat org.apache.spark.streaming.Checkpoint$$anonfun$serialize$1.apply$mcV$sp(Checkpoint.scala:152)\n\tat org.apache.spark.streaming.Checkpoint$$anonfun$serialize$1.apply(Checkpoint.scala:152)\n\tat org.apache.spark.streaming.Checkpoint$$anonfun$serialize$1.apply(Checkpoint.scala:152)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1363)\n\tat org.apache.spark.streaming.Checkpoint$.serialize(Checkpoint.scala:153)\n\tat org.apache.spark.streaming.StreamingContext.validate(StreamingContext.scala:525)\n\tat org.apache.spark.streaming.StreamingContext.liftedTree1$1(StreamingContext.scala:573)\n\tat org.apache.spark.streaming.StreamingContext.start(StreamingContext.scala:572)\n\tat org.apache.spark.streaming.api.java.JavaStreamingContext.start(JavaStreamingContext.scala:556)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: An exception was raised by Python:\nTraceback (most recent call last):\n  File \"/opt/cloudera/parcels/CDH-6.2.0-1.cdh6.2.0.p0.967373/lib/spark/python/pyspark/serializers.py\", line 587, in dumps\n    return cloudpickle.dumps(obj, 2)\n  File \"/opt/cloudera/parcels/CDH-6.2.0-1.cdh6.2.0.p0.967373/lib/spark/python/pyspark/cloudpickle.py\", line 863, in dumps\n    cp.dump(obj)\n  File \"/opt/cloudera/parcels/CDH-6.2.0-1.cdh6.2.0.p0.967373/lib/spark/python/pyspark/cloudpickle.py\", line 260, in dump\n    return Pickler.dump(self, obj)\n  File \"/usr/lib/python3.5/pickle.py\", line 408, in dump\n    self.save(obj)\n  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/usr/lib/python3.5/pickle.py\", line 725, in save_tuple\n    save(element)\n  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/opt/cloudera/parcels/CDH-6.2.0-1.cdh6.2.0.p0.967373/lib/spark/python/pyspark/cloudpickle.py\", line 406, in save_function\n    self.save_function_tuple(obj)\n  File \"/opt/cloudera/parcels/CDH-6.2.0-1.cdh6.2.0.p0.967373/lib/spark/python/pyspark/cloudpickle.py\", line 549, in save_function_tuple\n    save(state)\n  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/usr/lib/python3.5/pickle.py\", line 810, in save_dict\n    self._batch_setitems(obj.items())\n  File \"/usr/lib/python3.5/pickle.py\", line 836, in _batch_setitems\n    save(v)\n  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/usr/lib/python3.5/pickle.py\", line 770, in save_list\n    self._batch_appends(obj)\n  File \"/usr/lib/python3.5/pickle.py\", line 797, in _batch_appends\n    save(tmp[0])\n  File \"/usr/lib/python3.5/pickle.py\", line 520, in save\n    self.save_reduce(obj=obj, *rv)\n  File \"/usr/lib/python3.5/pickle.py\", line 623, in save_reduce\n    save(state)\n  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/usr/lib/python3.5/pickle.py\", line 810, in save_dict\n    self._batch_setitems(obj.items())\n  File \"/usr/lib/python3.5/pickle.py\", line 836, in _batch_setitems\n    save(v)\n  File \"/usr/lib/python3.5/pickle.py\", line 520, in save\n    self.save_reduce(obj=obj, *rv)\n  File \"/usr/lib/python3.5/pickle.py\", line 623, in save_reduce\n    save(state)\n  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/usr/lib/python3.5/pickle.py\", line 810, in save_dict\n    self._batch_setitems(obj.items())\n  File \"/usr/lib/python3.5/pickle.py\", line 836, in _batch_setitems\n    save(v)\n  File \"/usr/lib/python3.5/pickle.py\", line 495, in save\n    rv = reduce(self.proto)\n  File \"/opt/cloudera/parcels/CDH-6.2.0-1.cdh6.2.0.p0.967373/lib/spark/python/pyspark/context.py\", line 321, in __getnewargs__\n    \"It appears that you are attempting to reference SparkContext from a broadcast \"\nException: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/opt/cloudera/parcels/CDH-6.2.0-1.cdh6.2.0.p0.967373/lib/spark/python/pyspark/streaming/util.py\", line 115, in dumps\n    func.func, func.rdd_wrap_func, func.deserializers)))\n  File \"/opt/cloudera/parcels/CDH-6.2.0-1.cdh6.2.0.p0.967373/lib/spark/python/pyspark/serializers.py\", line 597, in dumps\n    raise pickle.PicklingError(msg)\n_pickle.PicklingError: Could not serialize object: Exception: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.\n\n\tat org.apache.spark.streaming.api.python.PythonTransformFunctionSerializer$.serialize(PythonDStream.scala:144)\n\tat org.apache.spark.streaming.api.python.TransformFunction$$anonfun$writeObject$1.apply$mcV$sp(PythonDStream.scala:101)\n\tat org.apache.spark.streaming.api.python.TransformFunction$$anonfun$writeObject$1.apply(PythonDStream.scala:100)\n\tat org.apache.spark.streaming.api.python.TransformFunction$$anonfun$writeObject$1.apply(PythonDStream.scala:100)\n\tat org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1329)\n\t... 61 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-aad8f8bc732b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;31m# Start the computation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m     \u001b[0mssc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m     \u001b[0mssc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/cloudera/parcels/CDH-6.2.0-1.cdh6.2.0.p0.967373/lib/spark/python/pyspark/streaming/context.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    180\u001b[0m         \u001b[0mStart\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mexecution\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mstreams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \"\"\"\n\u001b[0;32m--> 182\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jssc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m         \u001b[0mStreamingContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activeContext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/cloudera/parcels/CDH-6.2.0-1.cdh6.2.0.p0.967373/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/cloudera/parcels/CDH-6.2.0-1.cdh6.2.0.p0.967373/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o1731.start.\n: java.io.IOException: org.apache.spark.SparkException: An exception was raised by Python:\nTraceback (most recent call last):\n  File \"/opt/cloudera/parcels/CDH-6.2.0-1.cdh6.2.0.p0.967373/lib/spark/python/pyspark/serializers.py\", line 587, in dumps\n    return cloudpickle.dumps(obj, 2)\n  File \"/opt/cloudera/parcels/CDH-6.2.0-1.cdh6.2.0.p0.967373/lib/spark/python/pyspark/cloudpickle.py\", line 863, in dumps\n    cp.dump(obj)\n  File \"/opt/cloudera/parcels/CDH-6.2.0-1.cdh6.2.0.p0.967373/lib/spark/python/pyspark/cloudpickle.py\", line 260, in dump\n    return Pickler.dump(self, obj)\n  File \"/usr/lib/python3.5/pickle.py\", line 408, in dump\n    self.save(obj)\n  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/usr/lib/python3.5/pickle.py\", line 725, in save_tuple\n    save(element)\n  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/opt/cloudera/parcels/CDH-6.2.0-1.cdh6.2.0.p0.967373/lib/spark/python/pyspark/cloudpickle.py\", line 406, in save_function\n    self.save_function_tuple(obj)\n  File \"/opt/cloudera/parcels/CDH-6.2.0-1.cdh6.2.0.p0.967373/lib/spark/python/pyspark/cloudpickle.py\", line 549, in save_function_tuple\n    save(state)\n  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/usr/lib/python3.5/pickle.py\", line 810, in save_dict\n    self._batch_setitems(obj.items())\n  File \"/usr/lib/python3.5/pickle.py\", line 836, in _batch_setitems\n    save(v)\n  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/usr/lib/python3.5/pickle.py\", line 770, in save_list\n    self._batch_appends(obj)\n  File \"/usr/lib/python3.5/pickle.py\", line 797, in _batch_appends\n    save(tmp[0])\n  File \"/usr/lib/python3.5/pickle.py\", line 520, in save\n    self.save_reduce(obj=obj, *rv)\n  File \"/usr/lib/python3.5/pickle.py\", line 623, in save_reduce\n    save(state)\n  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/usr/lib/python3.5/pickle.py\", line 810, in save_dict\n    self._batch_setitems(obj.items())\n  File \"/usr/lib/python3.5/pickle.py\", line 836, in _batch_setitems\n    save(v)\n  File \"/usr/lib/python3.5/pickle.py\", line 520, in save\n    self.save_reduce(obj=obj, *rv)\n  File \"/usr/lib/python3.5/pickle.py\", line 623, in save_reduce\n    save(state)\n  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/usr/lib/python3.5/pickle.py\", line 810, in save_dict\n    self._batch_setitems(obj.items())\n  File \"/usr/lib/python3.5/pickle.py\", line 836, in _batch_setitems\n    save(v)\n  File \"/usr/lib/python3.5/pickle.py\", line 495, in save\n    rv = reduce(self.proto)\n  File \"/opt/cloudera/parcels/CDH-6.2.0-1.cdh6.2.0.p0.967373/lib/spark/python/pyspark/context.py\", line 321, in __getnewargs__\n    \"It appears that you are attempting to reference SparkContext from a broadcast \"\nException: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/opt/cloudera/parcels/CDH-6.2.0-1.cdh6.2.0.p0.967373/lib/spark/python/pyspark/streaming/util.py\", line 115, in dumps\n    func.func, func.rdd_wrap_func, func.deserializers)))\n  File \"/opt/cloudera/parcels/CDH-6.2.0-1.cdh6.2.0.p0.967373/lib/spark/python/pyspark/serializers.py\", line 597, in dumps\n    raise pickle.PicklingError(msg)\n_pickle.PicklingError: Could not serialize object: Exception: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.\n\n\tat org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1336)\n\tat org.apache.spark.streaming.api.python.TransformFunction.writeObject(PythonDStream.scala:100)\n\tat sun.reflect.GeneratedMethodAccessor19.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:1140)\n\tat java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1496)\n\tat java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)\n\tat java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)\n\tat java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)\n\tat java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)\n\tat java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)\n\tat java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)\n\tat java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)\n\tat java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)\n\tat java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)\n\tat java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)\n\tat java.io.ObjectOutputStream.writeArray(ObjectOutputStream.java:1378)\n\tat java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1174)\n\tat java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)\n\tat java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)\n\tat java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)\n\tat java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)\n\tat java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)\n\tat java.io.ObjectOutputStream.defaultWriteObject(ObjectOutputStream.java:441)\n\tat org.apache.spark.streaming.DStreamGraph$$anonfun$writeObject$1.apply$mcV$sp(DStreamGraph.scala:187)\n\tat org.apache.spark.streaming.DStreamGraph$$anonfun$writeObject$1.apply(DStreamGraph.scala:182)\n\tat org.apache.spark.streaming.DStreamGraph$$anonfun$writeObject$1.apply(DStreamGraph.scala:182)\n\tat org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1329)\n\tat org.apache.spark.streaming.DStreamGraph.writeObject(DStreamGraph.scala:182)\n\tat sun.reflect.GeneratedMethodAccessor66.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:1140)\n\tat java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1496)\n\tat java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)\n\tat java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)\n\tat java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)\n\tat java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)\n\tat java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)\n\tat java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)\n\tat java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348)\n\tat org.apache.spark.streaming.Checkpoint$$anonfun$serialize$1.apply$mcV$sp(Checkpoint.scala:152)\n\tat org.apache.spark.streaming.Checkpoint$$anonfun$serialize$1.apply(Checkpoint.scala:152)\n\tat org.apache.spark.streaming.Checkpoint$$anonfun$serialize$1.apply(Checkpoint.scala:152)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1363)\n\tat org.apache.spark.streaming.Checkpoint$.serialize(Checkpoint.scala:153)\n\tat org.apache.spark.streaming.StreamingContext.validate(StreamingContext.scala:525)\n\tat org.apache.spark.streaming.StreamingContext.liftedTree1$1(StreamingContext.scala:573)\n\tat org.apache.spark.streaming.StreamingContext.start(StreamingContext.scala:572)\n\tat org.apache.spark.streaming.api.java.JavaStreamingContext.start(JavaStreamingContext.scala:556)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: An exception was raised by Python:\nTraceback (most recent call last):\n  File \"/opt/cloudera/parcels/CDH-6.2.0-1.cdh6.2.0.p0.967373/lib/spark/python/pyspark/serializers.py\", line 587, in dumps\n    return cloudpickle.dumps(obj, 2)\n  File \"/opt/cloudera/parcels/CDH-6.2.0-1.cdh6.2.0.p0.967373/lib/spark/python/pyspark/cloudpickle.py\", line 863, in dumps\n    cp.dump(obj)\n  File \"/opt/cloudera/parcels/CDH-6.2.0-1.cdh6.2.0.p0.967373/lib/spark/python/pyspark/cloudpickle.py\", line 260, in dump\n    return Pickler.dump(self, obj)\n  File \"/usr/lib/python3.5/pickle.py\", line 408, in dump\n    self.save(obj)\n  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/usr/lib/python3.5/pickle.py\", line 725, in save_tuple\n    save(element)\n  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/opt/cloudera/parcels/CDH-6.2.0-1.cdh6.2.0.p0.967373/lib/spark/python/pyspark/cloudpickle.py\", line 406, in save_function\n    self.save_function_tuple(obj)\n  File \"/opt/cloudera/parcels/CDH-6.2.0-1.cdh6.2.0.p0.967373/lib/spark/python/pyspark/cloudpickle.py\", line 549, in save_function_tuple\n    save(state)\n  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/usr/lib/python3.5/pickle.py\", line 810, in save_dict\n    self._batch_setitems(obj.items())\n  File \"/usr/lib/python3.5/pickle.py\", line 836, in _batch_setitems\n    save(v)\n  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/usr/lib/python3.5/pickle.py\", line 770, in save_list\n    self._batch_appends(obj)\n  File \"/usr/lib/python3.5/pickle.py\", line 797, in _batch_appends\n    save(tmp[0])\n  File \"/usr/lib/python3.5/pickle.py\", line 520, in save\n    self.save_reduce(obj=obj, *rv)\n  File \"/usr/lib/python3.5/pickle.py\", line 623, in save_reduce\n    save(state)\n  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/usr/lib/python3.5/pickle.py\", line 810, in save_dict\n    self._batch_setitems(obj.items())\n  File \"/usr/lib/python3.5/pickle.py\", line 836, in _batch_setitems\n    save(v)\n  File \"/usr/lib/python3.5/pickle.py\", line 520, in save\n    self.save_reduce(obj=obj, *rv)\n  File \"/usr/lib/python3.5/pickle.py\", line 623, in save_reduce\n    save(state)\n  File \"/usr/lib/python3.5/pickle.py\", line 475, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"/usr/lib/python3.5/pickle.py\", line 810, in save_dict\n    self._batch_setitems(obj.items())\n  File \"/usr/lib/python3.5/pickle.py\", line 836, in _batch_setitems\n    save(v)\n  File \"/usr/lib/python3.5/pickle.py\", line 495, in save\n    rv = reduce(self.proto)\n  File \"/opt/cloudera/parcels/CDH-6.2.0-1.cdh6.2.0.p0.967373/lib/spark/python/pyspark/context.py\", line 321, in __getnewargs__\n    \"It appears that you are attempting to reference SparkContext from a broadcast \"\nException: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/opt/cloudera/parcels/CDH-6.2.0-1.cdh6.2.0.p0.967373/lib/spark/python/pyspark/streaming/util.py\", line 115, in dumps\n    func.func, func.rdd_wrap_func, func.deserializers)))\n  File \"/opt/cloudera/parcels/CDH-6.2.0-1.cdh6.2.0.p0.967373/lib/spark/python/pyspark/serializers.py\", line 597, in dumps\n    raise pickle.PicklingError(msg)\n_pickle.PicklingError: Could not serialize object: Exception: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.\n\n\tat org.apache.spark.streaming.api.python.PythonTransformFunctionSerializer$.serialize(PythonDStream.scala:144)\n\tat org.apache.spark.streaming.api.python.TransformFunction$$anonfun$writeObject$1.apply$mcV$sp(PythonDStream.scala:101)\n\tat org.apache.spark.streaming.api.python.TransformFunction$$anonfun$writeObject$1.apply(PythonDStream.scala:100)\n\tat org.apache.spark.streaming.api.python.TransformFunction$$anonfun$writeObject$1.apply(PythonDStream.scala:100)\n\tat org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1329)\n\t... 61 more\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import json\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "import json\n",
    "\n",
    "# Initialize SparkContext and StreamingContext with a 1-second batch interval\n",
    "app_name = \"Toots_per_Idioma\"  # Name of your application\n",
    "\n",
    "# Create the SparkContext\n",
    "try:\n",
    "    sc = SparkContext(\"local[2]\", appName = app_name)\n",
    "except ValueError:\n",
    "    sc.stop()\n",
    "    sc = SparkContext(\"local[2]\", appName = app_name)\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "\n",
    "# Create the StreamingContext\n",
    "batch_interval = 5  # Batch interval in seconds\n",
    "ssc = StreamingContext(sc, batch_interval)\n",
    "ssc.checkpoint(\"checkpoint\")  # Necessary for updateStateByKey operation\n",
    "\n",
    "# Define Kafka parameters\n",
    "kafka_server = \"Cloudera02:9092\"  # Kafka server address\n",
    "kafka_topic = \"activity2gmatav\"   # Kafka topic\n",
    "kafka_group = \"GrupStreaming5\"   # Kafka consumer group, first surname of each member of the group separated by an underscore.\n",
    "\n",
    "kafkaParams = {\n",
    "    \"metadata.broker.list\": kafka_server,\n",
    "    \"group.id\": kafka_group\n",
    "}\n",
    "\n",
    "# Create a DStream that connects to Kafka\n",
    "kafkaStream = KafkaUtils.createDirectStream(ssc, [kafka_topic], kafkaParams)\n",
    "\n",
    "# Count the number of toots per language\n",
    "tootLangCounts = kafkaStream\\\n",
    "    .map(lambda x: json.loads(x[1])) \\\n",
    "    .filter(lambda toot: not toot.get(\"reblog\", False)) \\\n",
    "    .map(lambda toot: (toot.get(\"language\", \"unknown\"), 1)) \\\n",
    "    .reduceByKeyAndWindow(lambda a, b: a + b, lambda a, b: a - b, 5, 5)\n",
    "\n",
    "\n",
    "# Print the cumulative count\n",
    "tootLangCounts.pprint(tootLangCounts \\\n",
    "    .transform(lambda rdd: rdd.sortBy(lambda x: x[1], ascending=False)) \\\n",
    "    .transform(lambda rdd: sc.parallelize(rdd.take(10))))\n",
    "\n",
    "                      \n",
    "# Start the computation       \n",
    "try:\n",
    "    ssc.start()  \n",
    "    ssc.awaitTermination()  \n",
    "except KeyboardInterrupt:\n",
    "    ssc.stop()\n",
    "    sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66669363",
   "metadata": {},
   "source": [
    "Adjunta la captura de pantalla de la sortida **aquí**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20739d91",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d919e8792a45cea8d2f66c0773b12f73",
     "grade": false,
     "grade_id": "cell-567bdf2c648dc8c5",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "### Exercici 10.3: Mantenint el Comptatge (0.5 punts)\n",
    "\n",
    "\n",
    "Fins ara, hem estat obtenint resultats específics de lots, la qual cosa és generalment útil. Però, què passa si vols obtenir una visió més àmplia, mantenint la informació a través de les finestres per, per exemple, acumular tendències al llarg del temps? Aquest és l'enfocament de la nostra pròxima exploració.\n",
    "\n",
    "En aquest exercici, et convidem a modificar l'script anterior per **mantenir un comptatge acumulat de tots els toots originals, categorizats per idioma**. En lloc de simplement comptar nous toots cada cinc segons, anem a **sumar-los contínuament**. Pensa en això com un marcador que s'actualitza constantment amb el nombre total de toots originals en cada idioma des del moment en què comencem a fer streaming.\n",
    "\n",
    "Per aconseguir-ho, treballarem amb les transformacions\n",
    "[**stateful transformations in Spark Streaming**](https://spark.apache.org/docs/2.4.0/streaming-programming-guide.html#caching--persistence).\n",
    "Això és una forma sofisticada de dir que recordarem les dades passades i les utilitzarem en els nostres càlculs actuals. És similar a mantenir un total acumulat en una variable global en lloc de començar des de zero cada vegada.\n",
    "\n",
    "**Et convidem a completar l'script següent:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f9b283a",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-2-1e2668bd79a1>, line 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-1e2668bd79a1>\"\u001b[0;36m, line \u001b[0;32m10\u001b[0m\n\u001b[0;31m    app_name = <FILLIN>  # Name of your application\u001b[0m\n\u001b[0m               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "import json\n",
    "\n",
    "# Initialize SparkContext and StreamingContext with a 1-second batch interval\n",
    "app_name = <FILLIN>  # Name of your application\n",
    "\n",
    "# Create the SparkContext\n",
    "try:\n",
    "    sc = SparkContext(\"local[2]\", appName=\"app_name\")\n",
    "except ValueError:\n",
    "    sc.stop()\n",
    "    sc = SparkContext(\"local[2]\", appName=\"app_name\")\n",
    "\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "\n",
    "batch_interval = 5  # Batch interval in seconds\n",
    "ssc = StreamingContext(<FILLIN>)\n",
    "ssc.checkpoint(\"checkpoint\")  # Necessary for updateStateByKey operation\n",
    "\n",
    "# Define Kafka parameters\n",
    "kafka_server = <FILLIN>  # Kafka server address\n",
    "kafka_topic = <FILLIN>   # Kafka topic\n",
    "kafka_group = <FILLIN>   # Kafka consumer group, first surname of each member of the group separated by an underscore.\n",
    "\n",
    "kafkaParams = {\n",
    "    \"metadata.broker.list\": kafka_server,\n",
    "    \"group.id\": kafka_group\n",
    "}\n",
    "\n",
    "\n",
    "# Create a DStream that connects to Kafka\n",
    "kafkaStream = KafkaUtils.createDirectStream(<FILLIN>)\n",
    "\n",
    "# Update the cumulative count using updateStateByKey\n",
    "def updateFunction(newValues, runningCount):\n",
    "    <FILLIN>\n",
    "    return <FILLIN>\n",
    "\n",
    "# Count each toot as 1 and update the total count\n",
    "tootCounts = kafkaStream\\\n",
    "    .map(lambda x: json.loads(x[1]))\\\n",
    "    .<FILLIN>\n",
    "    ....\n",
    "    .<FILLIN>\n",
    "    .updateStateByKey(<FILLIN>)\\\n",
    "    <FILLIN>\n",
    "\n",
    "# Print the cumulative count\n",
    "tootCounts.pprint()\n",
    "\n",
    "# Start the computation\n",
    "try:\n",
    "    ssc.start()\n",
    "    ssc.awaitTermination()\n",
    "except KeyboardInterrupt:\n",
    "    ssc.stop()\n",
    "    sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8d9158",
   "metadata": {},
   "source": [
    "Adjunta la captura de pantalla de la sortida **aquí**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03fe035",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "dc3e8fb932d72ec346d9979566a40b5d",
     "grade": false,
     "grade_id": "cell-b4f488f79d86b434",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "### Exercici 10.4: Windowed Counting (1 punt)\n",
    "\n",
    "Com has observat, Spark Streaming és increïblement flexible i fàcil d'utilitzar, i aquí tens un truc interessant que pot realitzar: **et permet trobar un punt intermedi entre comptar toots [time window](https://spark.apache.org/docs/2.4.0/streaming-programming-guide.html#window-operations) i mantenir un total acumulat**. Imaginem que volem crear un panell de control, com un tauler, que mostri el nombre de toots realitzats en cada idioma. \n",
    "El gir és que **volem que aquesta actualització es produeixi cada 5 segons, però estem rastrejant els comptatges durant un minut complet.**\n",
    "\n",
    "Així, cada 5 segons, el nostre panell s'actualitza, proporcionant-nos l'últim comptatge acumulat durant un minut. És com tenir un marcador en viu que s'actualitza amb freqüència i també realitza un seguiment del que ha ocorregut en els últims 60 segons, no només en els últims 5. D'aquesta manera, obtens tant actualitzacions immediates com una vista més àmplia del que està succeint, tot al mateix temps. Mostra només els 10 principals idiomes.\n",
    "\n",
    "**Modifica l'script següent per aconseguir aquest objectiu:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0ff66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "import json\n",
    "\n",
    "# Initialize SparkContext and StreamingContext with a 1-second batch interval\n",
    "app_name = \"FILLIN\"  # Name of your application\n",
    "\n",
    "# Create the SparkContext\n",
    "try:\n",
    "    sc = SparkContext(\"local[2]\", appName=\"app_name\")\n",
    "except ValueError:\n",
    "    sc.stop()\n",
    "    sc = SparkContext(\"local[2]\", appName=\"app_name\")\n",
    "\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "\n",
    "ssc = StreamingContext(<FILLIN>)\n",
    "ssc.checkpoint(\"checkpoint\")  # Necessary for updateStateByKey operation\n",
    "\n",
    "# Define Kafka parameters\n",
    "kafka_server = <FILLIN>  # Kafka server address\n",
    "kafka_topic = <FILLIN>   # Kafka topic\n",
    "kafka_group = <FILLIN>         # Kafka consumer group, first surname of each member of the group separated by an underscore.\n",
    "\n",
    "kafkaParams = {\n",
    "    \"metadata.broker.list\": kafka_server,\n",
    "    \"group.id\": kafka_group\n",
    "}\n",
    "\n",
    "\n",
    "# Create a DStream that connects to Kafka\n",
    "kafkaStream = KafkaUtils.createDirectStream(<FILLIN> )\n",
    "\n",
    "# Count each toot as 1 and update the total count. Use a 60-second window with a 5-second slide\n",
    "tootCounts = kafkaStream\\\n",
    "    .<FILLIN>\n",
    "\n",
    "# Print the cumulative count\n",
    "tootCounts.pprint()\n",
    "\n",
    "# Start the computation\n",
    "try:\n",
    "    ssc.start()\n",
    "    ssc.awaitTermination()\n",
    "except KeyboardInterrupt:\n",
    "    ssc.stop()\n",
    "    sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03514a4b",
   "metadata": {},
   "source": [
    "Adjunta la captura de pantalla de la sortida **aquí**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e81f73",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "18d2bba9467f2531dfe6f1eac9e71f41",
     "grade": false,
     "grade_id": "cell-8a76c6c76b006e10",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "### Exercici 10.5: Powering Up (1 punt)\n",
    "\n",
    "Molt bé, ja sabem que els RDDs en Spark són increïblement versàtils: són com la navalla suïssa del processament de dades de stream. Pots fer pràcticament qualsevol operació amb ells. No obstant això, **a mesura que les coses es tornen més complexes, el desafiament augmenta.**\n",
    "\n",
    "Ara, farem que el nostre panell de control sigui encara més interessant. \n",
    "En lloc de només mostrar quants toots tenim per minut, afegim algunes característiques noves. No seria interessant **rastrejar la longitud mitjana d'aquests toots?**\n",
    "I hi ha més: anem a descobrir **qui és l'usuari més seguit entre els que han tootejat en aquest minut.**\n",
    "\n",
    "Espera, **hi ha més!** Per fer tota aquesta informació súper fàcil de llegir i entendre, la presentarem en un format de taula ordenat i net. No es tracta només de les dades, sinó de fer-les amigables per a l'usuari i visualment digeribles.\n",
    "\n",
    "La taula resultant s'ha d'actualitzar en intervals de 5 segons, i les finestres de mitjana han de ser de 60 segons. Les columnes d'aquesta taula han de ser:\n",
    "\n",
    "-   **`lang`:** Idioma\n",
    "-   **`num_toots`:** Nombre de toots originals en aquest idioma\n",
    "-   **`avg_len_content`:** Longitud mitjana (en caràcters) del contingut del toot\n",
    "-   **`user`:** Usuari més seguit\n",
    "-   **`followers`:** Nombre de seguidors d'aquest usuari\n",
    "\n",
    "Per fer que la sortida sigui més llegible, limita el nombre de files a 10.\n",
    "\n",
    "**TIP:** Hi ha un exemple molt útil a [Spark\n",
    "Streaming](https://spark.apache.org/docs/2.4.0/streaming-programming-guide.html#dataframe-and-sql-operations).. ¡Búsca'l!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c769f1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "<FILL IN>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1da10c",
   "metadata": {},
   "source": [
    "Adjunta la captura de pantalla de la sortida **aquí**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5d3514",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "825334a35954957c00975c7ba497fded",
     "grade": false,
     "grade_id": "cell-78a4d1f04421041f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Exercici 11: Structured Streaming (3 punts)\n",
    "\n",
    "Com has vist en l'últim exercici, depenent de les operacions, l'API de Spark Streaming pot no resultar tan convenient, especialment perquè has de tractar amb APIs de baix nivell. Afortunadament, **Spark provides a\n",
    "high-level API called [Spark Structured\n",
    "Streaming](https://spark.apache.org/docs/2.4.0/structured-streaming-programming-guide.html)** que et permet expressar els càlculs de streaming de la mateixa manera que expressaries un càlcul per lots sobre dades estructurades estàtiques, com les que faries servir en el processament per lots.\n",
    "\n",
    "En aquest conjunt d'exercicis, t'endinsaràs en el fascinant món de Spark Structured Streaming."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d938ae",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6f3260cf1c3edd45d07b51bd920acdbb",
     "grade": false,
     "grade_id": "cell-bb18ad1646897ea8",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "### Exercici 11.1: Obtenint l'Esquema (0.5 punts)\n",
    "\n",
    "Una de les característiques més interessants de Spark Structured Streaming és **com gestiona les dades estructurades**. Per exemple, el flux de dades en el nostre tema de Kafka, on **cada \"toot\" arriba en un format JSON ordenat**, cosa que el fa estructurat i organitzat.\n",
    "\n",
    "De manera similar a com es treballa amb DataFrames en Spark, **Structured Streaming utilitza esquemes de dades per analitzar dades estructurades**, essencialment un plànol de com estan disposades les dades. Per al processament per lots, Spark pot sovint deduir aquesta estructura directament de les dades. No obstant això, amb les dades de streaming, és una mica diferent: **necessitem definir aquesta estructura per endavant**.\n",
    "\n",
    "En els següents exercicis, utilitzarem un truc convenient: **en lloc de definir manualment** l'estructura complexa d'un \"toot\", inicialment **extraiem alguns toots de Kafka i els analitzem en un lot per aprendre el seu esquema**. És com fer una ullada per entendre com estan organitzades les coses. Un cop tinguem l'esquema, l'aplicarem al nostre pipeline de streaming.\n",
    "\n",
    "La teva tasca en aquest exercici és realitzar aquesta transformació. Després, utilitzant les operacions de DataFrame amb les quals ja estàs familiaritzat, crearàs una taula amb les següents columnes que ens permetrà veure els toots individualment a mesura que es processen:\n",
    "\n",
    "\n",
    "-   **`id`:** Identificador únic per a cada toot\n",
    "-   **`created_at`:** Data i hora en què es va crear el toot\n",
    "-   **`content`:** El contingut del toot\n",
    "-   **`language`:** L'idioma del toot\n",
    "-   **`username`:** El nomb d'usuari de l'autor del toot\n",
    "-   **`followers_count`:** Nombre de seguidors de l'autor.\n",
    "\n",
    "\n",
    "Recorda que ens **interessen els toots originals**. Filtra aquells que són retweets.\n",
    "\n",
    "Un altre aspecte fonamental que has de gestionar aquí és **seleccionar l'outputMode adequat**. Consulta la [the documentation](https://spark.apache.org/docs/2.4.0/structured-streaming-programming-guide.html#output-modes) i tria el que millor s'adapti a aquest exercici.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70fa6093",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StringType, StructType, StructField\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql.functions import json_tuple, from_json, col\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "conf = SparkConf()\n",
    "conf.setMaster(\"local[2]\")\n",
    "sc = SparkContext(conf=conf)\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "\n",
    "# Initialize Spark Session for Structured Streaming\n",
    "app_name = \"activity3_1\" + <FILLIN> # Replace with your Spark app name must include the username of the members of the group\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(app_name) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Define Kafka parameters\n",
    "kafka_topic = <FILLIN>\n",
    "kafka_bootstrap_servers = <FILLIN>  # Replace with your Kafka bootstrap servers\n",
    "\n",
    "# Read a small batch of data from Kafka for schema inference!\n",
    "batch_df = spark \\\n",
    "    .read \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", kafka_bootstrap_servers) \\\n",
    "    .option(\"subscribe\", kafka_topic) \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .option(\"endingOffsets\", \"{\\\"\" + kafka_topic + \"\\\":{\\\"0\\\":10}}\") \\\n",
    "    .load()\n",
    "\n",
    "# Infer schema\n",
    "schema = spark.read.json(batch_df.selectExpr(\"CAST(value AS STRING)\").rdd.map(lambda x: x[0])).schema\n",
    "\n",
    "# Create streaming DataFrame by reading data from Kafka\n",
    "toots = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", kafka_bootstrap_servers) \\\n",
    "    .option(\"subscribe\", kafka_topic) \\\n",
    "    .option(\"startingOffsets\", <FILLIN>) \\\n",
    "    .load()\n",
    "\n",
    "# Parse the value column as JSON and apply the infered schema. Then select the columns we need.\n",
    "toots_df = toots\\\n",
    "    .select(from_json(col(\"value\").cast(\"string\"), schema).alias(\"parsed_value\"))\\\n",
    "    .select(<FILLIN>)\n",
    "\n",
    "try:\n",
    "    # Open stream to console (you need to execute it in a terminal to see the output)\n",
    "    query = toots_df \\\n",
    "            .writeStream \\\n",
    "            .outputMode(<FILLIN>) \\\n",
    "            .format(\"console\")\\\n",
    "            .start()\n",
    "\n",
    "    query.awaitTermination()\n",
    "except KeyboardInterrupt:\n",
    "    query.stop()\n",
    "    spark.stop()\n",
    "    sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b639ae",
   "metadata": {},
   "source": [
    "Adjunta la captura de pantalla de la sortida **aquí**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58af1b1",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bfdf67aeb5c499e2f966d4d014de6ec8",
     "grade": false,
     "grade_id": "cell-6646f36d2cf2c16b",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "### Exercici 11.2: Agregant Dades des d'un Flux (0.5 punts)\n",
    "\n",
    "\n",
    "Spark Structured Streaming és realment potent, especialment quan es realitzen operacions sobre un flux continu de dades. En aquest exercici, aprofundirem en Structured Spark Streaming, enfocant-nos específicament en l'agregació de dades des d'un flux de Kafka. És similar al que vam fer en l'Exercici 2. La teva missió és **comptar el nombre de toots originals en cada idioma.**\n",
    "\n",
    "Aquí et mostrem com hauria de ser la teva sortida:\n",
    "\n",
    "-   **`language`:** Aquesta columna mostra l'idioma dels toots.\n",
    "-   **`count`:** Aquí és on mostraràs el nombre de toots per cada idioma.\n",
    "\n",
    "La teva taula **ha d'acumular aquests comptatges cada 10 segons, i també ha de seguir acumulant-los**. A més, fes-la amigable per a l'usuari **ordenant els idiomes pel nombre de toots, amb els idiomes més conversadors a la part superior.**\n",
    "\n",
    "Ara, aquí hi ha una **part clau** d'aquest exercici: **necessites triar el mode de sortida adequat per a la teva consulta de streaming**. Recorda, el mode de **sortida determina com s'escriu cada lot de dades resultant en la destinació de sortida**. Les teves opcions són els modes 'Complete', 'Append' i 'Update'. Pensa quin encaixa millor per al nostre escenari de comptatge acumulatiu i ordenat. I **no oblidis escriure el teu raonament en els comentaris.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ac3700",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StringType, StructType, StructField\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql.functions import from_json, col, window\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "conf = SparkConf()\n",
    "conf.setMaster(\"local[2]\")\n",
    "sc = SparkContext(conf=conf)\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "\n",
    "# Initialize Spark Session for Structured Streaming\n",
    "app_name = \"activity3_2\" + <FILLIN> # Replace with your Spark app name must include the username of the members of the group\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(app_name) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Define Kafka parameters\n",
    "kafka_topic = <FILLIN>\n",
    "kafka_bootstrap_servers = <FILLIN>  # Replace with your Kafka bootstrap servers\n",
    "\n",
    "# Read a small batch of data from Kafka for schema inference!\n",
    "batch_df = spark \\\n",
    "    .read \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", kafka_bootstrap_servers) \\\n",
    "    .option(\"subscribe\", kafka_topic) \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .option(\"endingOffsets\", \"{\\\"\" + kafka_topic + \"\\\":{\\\"0\\\":10}}\") \\\n",
    "    .load()\n",
    "\n",
    "# Infer schema\n",
    "schema = spark.read.json(batch_df.selectExpr(\"CAST(value AS STRING)\").rdd.map(lambda x: x[0])).schema\n",
    "\n",
    "# Create streaming DataFrame by reading data from Kafka\n",
    "toots = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", kafka_bootstrap_servers) \\\n",
    "    .option(\"subscribe\", kafka_topic) \\\n",
    "    .option(\"startingOffsets\", <FILLIN>) \\\n",
    "    .load()\n",
    "\n",
    "# Parse the value column as JSON and apply the infered schema. Then select the columns we need.\n",
    "toots_df = toots\\\n",
    "    .select(from_json(col(\"value\").cast(\"string\"), schema).alias(\"parsed_value\"))\\\n",
    "    .filter(<FILLIN>)\\\n",
    "    .select(<FILLIN>)\\\n",
    "    .groupBy(<FILLIN>)\\\n",
    "    .<FILLIN>\n",
    "    ...\n",
    "    <FILLIN>\n",
    "\n",
    "\n",
    "try:\n",
    "    # Open stream to console (you need to execute it in a terminal to see the output)\n",
    "    query = toots_df \\\n",
    "            .writeStream \\\n",
    "            .outputMode(<FILLIN>) \\\n",
    "            .format(\"console\")\\\n",
    "            .<FILLIN>\n",
    "            ...\n",
    "            .<FILLIN>\n",
    "            .trigger(<FILLIN>)\\\n",
    "            .start()\\\n",
    "\n",
    "    query.awaitTermination()\n",
    "except KeyboardInterrupt:\n",
    "    query.stop()\n",
    "    spark.stop()\n",
    "    sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e862fe",
   "metadata": {},
   "source": [
    "Adjunta la captura de pantalla de la sortida **aquí**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88cf8519",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8a99860919efd95259c3deef336bb119",
     "grade": false,
     "grade_id": "cell-42a4a46bcc6a639c",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "### Exercici 11.3: Windowed Counting (1 punt)\n",
    "\n",
    "Bona feina! Has après a realitzar agregacions i a fer un seguiment dels comptatges al llarg del temps. Com vas notar a l'Exercici 2.4, de vegades és més efectiu mantenir aquests comptatges en **specific time windows**. Ara, volem que apliquis aquesta tècnica utilitzant [functions available in Spark Structured\n",
    "Streaming](https://spark.apache.org/docs/2.4.0/structured-streaming-programming-guide.html#window-operations-on-event-time). \n",
    "Tingues en compte que Spark Structured Streaming gestiona el temps de manera diferent de Spark Streaming, per la qual cosa hauràs de considerar això a l'hora d'analitzar i interpretar els resultats.\n",
    "\n",
    "La teva tasca és **crear una taula que mostri un comptatge de la quantitat de toots originals (recorda filtrar els retweets) realitzats en cada idioma, segmentats dins d'un marc de temps específic**. Per a aquest exercici, has d'utilitzar una finestra lliscant d'un minut, amb les dades refrescant-se cada 5 segons. Aquest enfocament et permetrà monitorar de prop la freqüència dels toots en diferents idiomes al llarg d'intervals breus i superposats.\n",
    "\n",
    "Et demanem que proporcionis una taula amb l'estructura següent:\n",
    "\n",
    "- **`window`:** Mostra el rang de temps\n",
    "- **`language`:** Aquesta columna mostra l'idioma dels toots\n",
    "- **`count`:** Aquí és on mostraràs el nombre de toots per a cada idioma\n",
    "\n",
    "Els resultats han de **ordenar-se per finestra de temps i comptatge en ordre descendent.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6adaf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StringType, StructType, StructField\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql.functions import json_tuple, from_json, col, window\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "conf = SparkConf()\n",
    "conf.setMaster(\"local[2]\")\n",
    "sc = SparkContext(conf=conf)\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "\n",
    "# Initialize Spark Session for Structured Streaming\n",
    "app_name = \"activity3_3\" + <FILLIN> # Replace with your Spark app name must include the username of the members of the group\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(app_name) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Define Kafka parameters\n",
    "kafka_topic = <FILLIN>\n",
    "kafka_bootstrap_servers = <FILLIN>  # Replace with your Kafka bootstrap servers\n",
    "\n",
    "# Read a small batch of data from Kafka for schema inference!\n",
    "batch_df = spark \\\n",
    "    .read \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", kafka_bootstrap_servers) \\\n",
    "    .option(\"subscribe\", kafka_topic) \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .option(\"endingOffsets\", \"{\\\"\" + kafka_topic + \"\\\":{\\\"0\\\":10}}\") \\\n",
    "    .load()\n",
    "\n",
    "# Infer schema\n",
    "schema = spark.read.json(batch_df.selectExpr(\"CAST(value AS STRING)\").rdd.map(lambda x: x[0])).schema\n",
    "\n",
    "# Create streaming DataFrame by reading data from Kafka\n",
    "toots = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", kafka_bootstrap_servers) \\\n",
    "    .option(\"subscribe\", kafka_topic) \\\n",
    "    .option(\"startingOffsets\", <FILLIN>) \\\n",
    "    .load()\n",
    "\n",
    "# Parse the value column as JSON and apply the infered schema. Then select the columns we need.\n",
    "toots_df = toots\\\n",
    "    .select(from_json(col(\"value\").cast(\"string\"), schema).alias(\"parsed_value\"))\\\n",
    "    .filter(<FILLIN>)\\\n",
    "    .select(<FILLIN>)\\\n",
    "    .groupBy(\n",
    "        window(<FILLIN>),\n",
    "        <FILLIN>\n",
    "    )\\\n",
    "    .count()\\\n",
    "    .orderBy(<FILLIN>, <FILLIN>, ascending=False)\n",
    "\n",
    "try:\n",
    "    # Open stream to console (you need to execute it in a terminal to see the output)\n",
    "    query = toots_df \\\n",
    "            .writeStream \\\n",
    "            .outputMode(<FILLIN>) \\\n",
    "            .format(\"console\")\\\n",
    "            .option(\"truncate\", \"false\")\\\n",
    "            .trigger(<FILLIN>)\\\n",
    "            .start()\\\n",
    "\n",
    "    query.awaitTermination()\n",
    "except KeyboardInterrupt:\n",
    "    query.stop()\n",
    "    spark.stop()\n",
    "    sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040630ff",
   "metadata": {},
   "source": [
    "Adjunta la captura de pantalla de la sortida **aquí**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f9273a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "dac43eae3b742a2945c916ccbb53f47a",
     "grade": false,
     "grade_id": "cell-14149db32b69fa0d",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "### Exercici 11.4: Unir Fluxes (1 punt)\n",
    "\n",
    "En aquest últim exercici, explorarem una característica molt interessant de Spark Streaming que et permet **[unir dos streams](https://spark.apache.org/docs/2.4.0/structured-streaming-programming-guide.html#stream-stream-joins) i analitzar-los!**\n",
    "\n",
    "Per simplificar les coses, ja et proporcionem dos fluxos de dades pre-agregats.\n",
    "El primer, que es troba al tema de Kafka **`mastodon_toots_original_domain`**, mostra el comptatge de toots originals per a diverses instàncies de Mastodon (recorda que Mastodon és una federació d’instàncies) durant **fixed one-minute window**. \n",
    "\n",
    "El segon flux, en el **`mastodon_toots_retoot_domain` topic**, presenta dades similars, però per als toots que són retweets (compartits) d’altres toots. Les dades emmagatzemades en els temes de Kafka tenen la mateixa estructura en format JSON:\n",
    "\n",
    "- Una `window` estructura con dos elements `string` type: `start` i `end` \n",
    "- Un component `string` anomenat `mastodon_instance` amb el topic.\n",
    "- Un element de tipus `integer` anomenat `count`amb el nombre de toots realitzats en aquest domini en el rang de     temps específic\n",
    "\n",
    "Com que l’estructura de les dades és força senzilla, aquesta vegada **et demanem que la defineixis manualment**, en lloc d’utilitzar el truc de batch. Un cop hagis configurat les estructures, **hauràs d’obrir un flux per a cada font de Kafka.** El següent pas és unir aquests fluxos. Volem que facis un **left join del flux de toots originals amb el flux de retweets.** Un cop completada la unió, la teva sortida ha d’incloure:\n",
    "\n",
    "- **`window`:** el rang de temps\n",
    "- **`mastodon_instance`:** el domini de la instància de Mastodon\n",
    "- **`original_count`:** nombre de toots originals publicats en aquest domini durant aquest rang de temps\n",
    "- **`retweet_count`:** nombre de toots de retweets publicats en aquest domini durant aquest rang de temps\n",
    "\n",
    "\n",
    "**TIP**: Realitzar una unió en línia de dos fluxos no és una tasca fàcil, i hi ha moltes restriccions que has de respectar.\n",
    "[the\n",
    "documentation](https://spark.apache.org/docs/2.4.0/structured-streaming-programming-guide.html#stream-stream-joins)\n",
    "y recorda que estem utilitzant la versió 2.4.0. \n",
    "A més, recorda que estem realitzant **una join over time**, i aquest és un component **clau**. Conceptes com els que has après sobre windows són fonamentals juntament amb conceptes com [with concepts like\n",
    "watermarking](https://spark.apache.org/docs/2.4.0/structured-streaming-programming-guide.html#stream-stream-joins).\n",
    "I recorda, **els modes de sortida són complicats**, i has de triar-ne un que sigui adequat per a la tasca que vols fer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b59e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StringType, StructType, StructField, IntegerType\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql.functions import from_json, col, window, to_timestamp, struct\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark Context\n",
    "conf = SparkConf()\n",
    "conf.setMaster(\"local[2]\")\n",
    "sc = SparkContext(conf=conf)\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "\n",
    "# Initialize Spark Session for Structured Streaming\n",
    "app_name = \"activity3_4\" + <FILLIN> # Replace with your Spark app name must include the username of the members of the group\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(app_name) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Define schema for the incoming data\n",
    "schema = StructType(<FILLIN>)\n",
    "\n",
    "# Define Kafka parameters\n",
    "toots_original_topic = <FILLIN>\n",
    "toots_retoot_topic = <FILLIN>\n",
    "kafka_bootstrap_servers = <FILLIN>  # Replace with your Kafka bootstrap servers\n",
    "\n",
    "# Create streaming DataFrame by reading original toots data from Kafka\n",
    "toots_original = spark \\\n",
    "    .readStream \\\n",
    "    .<FILLIN>\n",
    "    ...\n",
    "    .<FILLIN>\n",
    "    .load()\n",
    "\n",
    "# Parse the value column as JSON and apply the infered schema. Then select the columns we need.\n",
    "toots_original_df = toots_original\\\n",
    "    .<FILLIN>\n",
    "    ...\n",
    "    .<FILLIN>\n",
    "\n",
    "# Create streaming DataFrame by reading retoots data from Kafka\n",
    "toots_retoot = spark \\\n",
    "    .readStream \\\n",
    "    .<FILLIN>\n",
    "    ...\n",
    "    .<FILLIN>\n",
    "    .load()\n",
    "\n",
    "# Parse the value column as JSON and apply the infered schema. Then select the columns we need.\n",
    "toots_retoot_df = toots_retoot\\\n",
    "    .<FILLIN>\n",
    "    ...\n",
    "    .<FILLIN>\n",
    "\n",
    "# Join the two streams\n",
    "toots_join_df = toots_original_df.join(<FILLIN>...<FILLIN>)\n",
    "\n",
    "try:\n",
    "    # Start running the query that prints the running counts to the console\n",
    "    query = toots_join_df\\\n",
    "            .writeStream \\\n",
    "            <FILLIN>\n",
    "            ...\n",
    "            <FILLIN>\n",
    "            .option(\"numRows\", 100)\\\n",
    "            .start()\\\n",
    "\n",
    "    query.awaitTermination()\n",
    "except KeyboardInterrupt:\n",
    "    query.stop()\n",
    "    spark.stop()\n",
    "    sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4241decc",
   "metadata": {},
   "source": [
    "Adjunta la captura de pantalla de la sortida **aquí**"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
